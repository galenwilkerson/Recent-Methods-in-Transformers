# Recent Methods in Transformers

This repository contains the Jupyter notebook "Recent Methods in Transformers.ipynb" which provides an overview of cutting-edge techniques in transformer models. The notebook includes theoretical explanations, practical code examples, and discussions on the implications of these new methods in the field of natural language processing.

## Repository Contents

- **Recent Methods in Transformers.ipynb**: A Jupyter notebook that dives deep into recent advancements in transformer technology. It covers various topics including FlashAttention, MatMul-Free Transformers, and more, providing both high-level overviews and detailed technical implementations.

## Getting Started

To get started with this repository, you will need to have Python installed, along with Jupyter Notebook or JupyterLab to run the notebook. It's also recommended to have a virtual environment set up.

### Prerequisites

- Python 3.8+
- Jupyter Notebook or JupyterLab
- Required Python libraries:
  ```bash
  pip install torch transformers numpy matplotlib
  ```

### Running the Notebook

1. Clone the repository:
   ```bash
   git clone https://github.com/galenwilkerson/Recent-Methods-in-Transformers.git
   ```
2. Navigate to the repository directory:
   ```bash
   cd Recent-Methods-in-Transformers
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Launch Jupyter Notebook or JupyterLab:
   ```bash
   jupyter notebook
   # or
   jupyter lab
   ```
5. Open the `Recent Methods in Transformers.ipynb` notebook and run the cells.

## Features

- **FlashAttention**: Explore the optimization of attention mechanisms for improved efficiency and performance.
- **MatMul-Free Transformers**: Learn about reducing computational overhead by eliminating traditional matrix multiplication.
- **Interactive Code**: Each section includes executable code blocks that allow for hands-on learning and experimentation.

## Contributing

Contributions to this project are welcome! Please feel free to fork the repository, make your changes, and submit a pull request. For major changes, please open an issue first to discuss what you would like to change.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Thanks to all the contributors who have invested their time in improving transformer technologies.
- Special thanks to those who have provided public datasets and libraries that facilitate research and development in AI.
