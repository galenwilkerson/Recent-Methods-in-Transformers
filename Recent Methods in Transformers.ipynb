{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bae49ea",
   "metadata": {},
   "source": [
    "# Recent Methods in Transformers\n",
    "\n",
    "Here are some of the latest methods in transformers, explained with intuitions, a bit of math, and simple documented implementations where possible:\n",
    "\n",
    "### 1. FlashAttention\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2205.14135\n",
    "\n",
    "**Intuition**: Think of FlashAttention as a way to make the process of finding which words in a sentence should be focused on faster and more efficient. It's like making sure your computer doesn't waste time and memory when figuring out which parts of your essay are most important.\n",
    "\n",
    "**Math**: FlashAttention reduces the complexity of the attention mechanism from $O(n^2)$ to $O(n)$ in terms of memory usage. It does this by breaking down the input data into smaller, more manageable pieces (tiling) and using efficient memory access patterns (kernel fusion).\n",
    "\n",
    "For a detailed implementation and tutorial, check out the [Flash-Attention-Tutorial repository](https://github.com/galenwilkerson/Flash-Attention-Tutorial).\n",
    "\n",
    "### 2. MatMul-Free Transformers\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2406.02528\n",
    "\n",
    "**Intuition**: Imagine instead of using complex calculations to multiply large matrices (like multiplying huge grids of numbers), you just add or subtract simple numbers like -1, 0, and 1. This makes the calculations much faster and easier for computers to handle.\n",
    "\n",
    "**Math**: This method uses ternary weights (-1, 0, 1) instead of full-precision weights and replaces matrix multiplication (MatMul) with addition and negation operations, which are simpler and faster.\n",
    "\n",
    "**Simple Implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea5d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0376,  0.1829,  1.3351],\n",
      "        [-1.5252, -1.4622, -2.4705],\n",
      "        [-0.3266,  0.9343, -0.6891],\n",
      "        [-0.5586, -1.4840, -2.7908],\n",
      "        [ 0.1397, -0.3241, -1.4200],\n",
      "        [-0.0884, -2.0239, -1.2238],\n",
      "        [-0.6058,  2.7050,  5.5249],\n",
      "        [-2.3538, -0.6834, -0.4525],\n",
      "        [-1.2831,  0.4118,  0.6496],\n",
      "        [-3.7471, -1.4224, -1.8167]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TernaryLinear(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TernaryLinear, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.randint(-1, 2, (input_dim, output_dim)).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weights)\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(10, 5)\n",
    "layer = TernaryLinear(5, 3)\n",
    "output = layer(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c837b51",
   "metadata": {},
   "source": [
    "### 3. The Mamba Architecture\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2312.00752\n",
    "\n",
    "**Intuition**: The Mamba architecture is like a very efficient and organized way to handle long pieces of text or data by remembering important parts and forgetting less important ones, making it much faster to process.\n",
    "\n",
    "**Math**: Mamba uses structured state space models (SSMs) that selectively propagate relevant information and scale linearly with input length, $O(n)$.\n",
    "\n",
    "### 4. Tandem Transformers\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2402.08644\n",
    "\n",
    "**Intuition**: Think of having two workers: the first one does the main job, and the second one checks and improves on blocks of the work done by the first. This teamwork makes the process more efficient.\n",
    "\n",
    "**Math**: Tandem Transformers involve a primary model that processes the input sequence, and a secondary model that processes blocks of tokens using representations from the primary model. This setup reduces computational requirements and improves inference efficiency.\n",
    "\n",
    "### 5. Advanced Positional Embeddings (ALiBi and RoPE)\n",
    "\n",
    "Papers: \n",
    "\n",
    "https://arxiv.org/abs/2108.12409  \n",
    "\n",
    "https://arxiv.org/abs/2310.13017 \n",
    "\n",
    "**Intuition**: Advanced positional embeddings help the transformer model understand the order of words in a sentence better by adding special numbers to the word representations, making it easier to capture the sequence information.\n",
    "\n",
    "**Math**: \n",
    "- **ALiBi**: Adds a linear bias to the attention scores based on the distance between tokens.\n",
    "- **RoPE**: Uses rotations of embeddings to encode relative positions, maintaining rotational invariance.\n",
    "\n",
    "**Simple Implementation for ALiBi**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9b7ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          ...,\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888]],\n",
      "\n",
      "         [[ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          ...,\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071]],\n",
      "\n",
      "         [[ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          ...,\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411]],\n",
      "\n",
      "         [[ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          ...,\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002]]],\n",
      "\n",
      "\n",
      "        [[[-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          ...,\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343]],\n",
      "\n",
      "         [[ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          ...,\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577]],\n",
      "\n",
      "         [[ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          ...,\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808]],\n",
      "\n",
      "         [[ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          ...,\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def alibi_attention(Q, K, V, bias):\n",
    "    \"\"\"\n",
    "    ALiBi Attention implementation: adding linear biases to attention scores.\n",
    "    Args:\n",
    "        Q: Queries matrix (batch_size, num_heads, seq_length, depth)\n",
    "        K: Keys matrix (batch_size, num_heads, seq_length, depth)\n",
    "        V: Values matrix (batch_size, num_heads, seq_length, depth)\n",
    "        bias: Linear bias matrix (num_heads, seq_length, seq_length)\n",
    "    Returns:\n",
    "        Output matrix (batch_size, num_heads, seq_length, depth)\n",
    "    \"\"\"\n",
    "    scores = torch.einsum('bhqd, bhkd -> bhqk', Q, K) + bias\n",
    "    attention = F.softmax(scores, dim=-1)\n",
    "    output = torch.einsum('bhqk, bhvd -> bhqd', attention, V)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "batch_size, num_heads, seq_length, depth = 2, 4, 8, 16\n",
    "Q = torch.randn(batch_size, num_heads, seq_length, depth)\n",
    "K = torch.randn(batch_size, num_heads, seq_length, depth)\n",
    "V = torch.randn(batch_size, num_heads, seq_length, depth)\n",
    "bias = torch.randn(num_heads, seq_length, seq_length)\n",
    "\n",
    "output = alibi_attention(Q, K, V, bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb47419",
   "metadata": {},
   "source": [
    "### 6. Prompt Learning\n",
    "\n",
    "Paper: https://arxiv.org/abs/2001.07676\n",
    "\n",
    "**Intuition**: Prompt learning is like giving the model a hint or a specific way to answer a question or complete a task, which helps it perform better without needing to be retrained from scratch.\n",
    "\n",
    "**Math**: Involves designing effective prompts that guide the model to produce the desired output. This can be seen as conditioning the model's responses on specific input patterns.\n",
    "\n",
    "**Simple Implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830886f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate English to French: Hello, how are you?\n",
      "\n",
      "Hello, how are you? Translate English to Spanish: Hello, how are you?\n",
      "\n",
      "Hello, how are you? Translate English to Portuguese: Hello, how are\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "prompt = \"Translate English to French: Hello, how are you?\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f428ba6",
   "metadata": {},
   "source": [
    "These methods represent the forefront of research and development in transformer models, focusing on improving efficiency, scalability, and performance in various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96df37a",
   "metadata": {},
   "source": [
    "### 7. Low-Rank Adaptation (LoRA)\n",
    "\n",
    "Paper: https://arxiv.org/abs/2106.09685\n",
    "\n",
    "LoRA introduces low-rank matrices to transformer models to adapt pre-trained weights effectively with minimal alterations to the model's architecture. This method significantly reduces the number of trainable parameters, making fine-tuning large models more computationally efficient. LoRA is particularly beneficial in scenarios where deploying lightweight models is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74ac655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, model_dim, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        # Initialize low-rank matrices A and B\n",
    "        self.A = nn.Parameter(torch.randn(model_dim, rank))\n",
    "        self.B = nn.Parameter(torch.randn(rank, model_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply low-rank matrices to input x\n",
    "        low_rank_interaction = self.A @ self.B\n",
    "        return x + x @ low_rank_interaction\n",
    "\n",
    "# Example usage of the LoRA layer\n",
    "model_dim, rank = 768, 32\n",
    "lora_layer = LoRALayer(model_dim, rank)\n",
    "input_tensor = torch.randn(1, model_dim)\n",
    "output = lora_layer(input_tensor)\n",
    "print(\"Output tensor shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd9cfb",
   "metadata": {},
   "source": [
    "### 8. Reformer\n",
    "\n",
    "Paper: https://arxiv.org/abs/2001.04451\n",
    "\n",
    "The Reformer model rethinks the self-attention mechanism to optimize for memory efficiency and speed, making it suitable for processing very long sequences. It uses techniques such as locality-sensitive hashing to reduce the complexity of attention from quadratic to logarithmic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3640c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sequence shape: torch.Size([1, 8192, 1000])\n"
     ]
    }
   ],
   "source": [
    "!pip install reformer_pytorch",
    "from reformer_pytorch import ReformerLM\n",
    "import torch\n",
    "\n",
    "# Initialize a Reformer model\n",
    "model = ReformerLM(\n",
    "    num_tokens=1000,  # number of tokens (size of the vocabulary)\n",
    "    dim=512,        # dimensions of the model\n",
    "    depth=6,        # number of layers\n",
    "    heads=8,        # number of attention heads\n",
    "    max_seq_len=8192,  # maximum length of the input sequences\n",
    "    ff_dropout=0.1,    # feedforward dropout rate\n",
    "    causal=True        # whether the model should be causal or not\n",
    ")\n",
    "\n",
    "# Create a random input tensor of integer type\n",
    "input_seq = torch.randint(0, 10, (1, 8192)).long()  # ensure indices are integers\n",
    "output_seq = model(input_seq)\n",
    "print(\"Output sequence shape:\", output_seq.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3aa90",
   "metadata": {},
   "source": [
    "### 9. Linformer\n",
    "\n",
    "Paper: https://arxiv.org/abs/2006.04768\n",
    "\n",
    "Linformer projects the self-attention mechanism's key and value matrices into a lower-dimensional space, thus reducing the memory and computational requirements. It's designed for handling long sequences more efficiently than standard transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3496a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "!pip install linformer\n",
    "from linformer import Linformer\n",
    "\n",
    "# Initialize a Linformer model\n",
    "linformer = Linformer(\n",
    "    #input_size = 512, # size of each token\n",
    "    #channels = 256,  # dimension of the mode\n",
    "    seq_len = 512,\n",
    "    dim = 256,\n",
    "    depth = 6,       # number of layers\n",
    "    k = 256,         # context window size\n",
    "    heads = 8,       # number of attention heads\n",
    "    dropout = 0.1\n",
    ")\n",
    "\n",
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 512, 256)  # batch size, sequence length, dimensions\n",
    "output_tensor = linformer(input_tensor)\n",
    "print(\"Output tensor shape:\", output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7b037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
