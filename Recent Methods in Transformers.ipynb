{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bae49ea",
   "metadata": {},
   "source": [
    "# Recent Methods in Transformers\n",
    "\n",
    "Here are some of the latest methods in transformers, explained with intuitions, a bit of math, and simple documented implementations where possible:\n",
    "\n",
    "### 1. FlashAttention\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2205.14135\n",
    "\n",
    "**Intuition**: Think of FlashAttention as a way to make the process of finding which words in a sentence should be focused on faster and more efficient. It's like making sure your computer doesn't waste time and memory when figuring out which parts of your essay are most important.\n",
    "\n",
    "**Math**: FlashAttention reduces the complexity of the attention mechanism from $O(n^2)$ to $O(n)$ in terms of memory usage. It does this by breaking down the input data into smaller, more manageable pieces (tiling) and using efficient memory access patterns (kernel fusion).\n",
    "\n",
    "For a detailed implementation and tutorial, check out the [Flash-Attention-Tutorial repository](https://github.com/galenwilkerson/Flash-Attention-Tutorial).\n",
    "\n",
    "### 2. MatMul-Free Transformers\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2406.02528\n",
    "\n",
    "**Intuition**: Imagine instead of using complex calculations to multiply large matrices (like multiplying huge grids of numbers), you just add or subtract simple numbers like -1, 0, and 1. This makes the calculations much faster and easier for computers to handle.\n",
    "\n",
    "**Math**: This method uses ternary weights (-1, 0, 1) instead of full-precision weights and replaces matrix multiplication (MatMul) with addition and negation operations, which are simpler and faster.\n",
    "\n",
    "**Simple Implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea5d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0376,  0.1829,  1.3351],\n",
      "        [-1.5252, -1.4622, -2.4705],\n",
      "        [-0.3266,  0.9343, -0.6891],\n",
      "        [-0.5586, -1.4840, -2.7908],\n",
      "        [ 0.1397, -0.3241, -1.4200],\n",
      "        [-0.0884, -2.0239, -1.2238],\n",
      "        [-0.6058,  2.7050,  5.5249],\n",
      "        [-2.3538, -0.6834, -0.4525],\n",
      "        [-1.2831,  0.4118,  0.6496],\n",
      "        [-3.7471, -1.4224, -1.8167]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TernaryLinear(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TernaryLinear, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.randint(-1, 2, (input_dim, output_dim)).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weights)\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(10, 5)\n",
    "layer = TernaryLinear(5, 3)\n",
    "output = layer(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c837b51",
   "metadata": {},
   "source": [
    "### 3. The Mamba Architecture\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2312.00752\n",
    "\n",
    "**Intuition**: The Mamba architecture is like a very efficient and organized way to handle long pieces of text or data by remembering important parts and forgetting less important ones, making it much faster to process.\n",
    "\n",
    "**Math**: Mamba uses structured state space models (SSMs) that selectively propagate relevant information and scale linearly with input length, $O(n)$.\n",
    "\n",
    "### 4. Tandem Transformers\n",
    "\n",
    "Paper:  https://arxiv.org/abs/2402.08644\n",
    "\n",
    "**Intuition**: Think of having two workers: the first one does the main job, and the second one checks and improves on blocks of the work done by the first. This teamwork makes the process more efficient.\n",
    "\n",
    "**Math**: Tandem Transformers involve a primary model that processes the input sequence, and a secondary model that processes blocks of tokens using representations from the primary model. This setup reduces computational requirements and improves inference efficiency.\n",
    "\n",
    "### 5. Advanced Positional Embeddings (ALiBi and RoPE)\n",
    "\n",
    "Papers: \n",
    "\n",
    "https://arxiv.org/abs/2108.12409  \n",
    "\n",
    "https://arxiv.org/abs/2310.13017 \n",
    "\n",
    "**Intuition**: Advanced positional embeddings help the transformer model understand the order of words in a sentence better by adding special numbers to the word representations, making it easier to capture the sequence information.\n",
    "\n",
    "**Math**: \n",
    "- **ALiBi**: Adds a linear bias to the attention scores based on the distance between tokens.\n",
    "- **RoPE**: Uses rotations of embeddings to encode relative positions, maintaining rotational invariance.\n",
    "\n",
    "**Simple Implementation for ALiBi**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9b7ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          ...,\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888],\n",
      "          [ 1.6854, -3.7547,  1.3007,  ..., -0.0780,  0.1969, -0.5888]],\n",
      "\n",
      "         [[ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          ...,\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071],\n",
      "          [ 2.0957,  3.3346, -1.3362,  ..., -0.8367, -0.6130,  1.0071]],\n",
      "\n",
      "         [[ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          ...,\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411],\n",
      "          [ 0.5329, -1.7503,  2.9328,  ...,  0.8077, -2.3167,  2.9411]],\n",
      "\n",
      "         [[ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          ...,\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002],\n",
      "          [ 0.6477, -0.8251,  3.2343,  ..., -1.2547, -0.3308, -4.3002]]],\n",
      "\n",
      "\n",
      "        [[[-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          ...,\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343],\n",
      "          [-2.0955, -4.8799,  0.6844,  ...,  0.2712,  2.0419,  0.9343]],\n",
      "\n",
      "         [[ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          ...,\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577],\n",
      "          [ 0.9531, -0.3947, -0.7437,  ..., -2.0960,  0.2970, -2.5577]],\n",
      "\n",
      "         [[ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          ...,\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808],\n",
      "          [ 2.6547,  0.8548,  4.4201,  ..., -0.8936, -1.6963, -3.2808]],\n",
      "\n",
      "         [[ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          ...,\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403],\n",
      "          [ 3.7743, -5.7084,  2.1409,  ...,  2.5845,  0.1297,  0.6403]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def alibi_attention(Q, K, V, bias):\n",
    "    \"\"\"\n",
    "    ALiBi Attention implementation: adding linear biases to attention scores.\n",
    "    Args:\n",
    "        Q: Queries matrix (batch_size, num_heads, seq_length, depth)\n",
    "        K: Keys matrix (batch_size, num_heads, seq_length, depth)\n",
    "        V: Values matrix (batch_size, num_heads, seq_length, depth)\n",
    "        bias: Linear bias matrix (num_heads, seq_length, seq_length)\n",
    "    Returns:\n",
    "        Output matrix (batch_size, num_heads, seq_length, depth)\n",
    "    \"\"\"\n",
    "    scores = torch.einsum('bhqd, bhkd -> bhqk', Q, K) + bias\n",
    "    attention = F.softmax(scores, dim=-1)\n",
    "    output = torch.einsum('bhqk, bhvd -> bhqd', attention, V)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "batch_size, num_heads, seq_length, depth = 2, 4, 8, 16\n",
    "Q = torch.randn(batch_size, num_heads, seq_length, depth)\n",
    "K = torch.randn(batch_size, num_heads, seq_length, depth)\n",
    "V = torch.randn(batch_size, num_heads, seq_length, depth)\n",
    "bias = torch.randn(num_heads, seq_length, seq_length)\n",
    "\n",
    "output = alibi_attention(Q, K, V, bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb47419",
   "metadata": {},
   "source": [
    "### 6. Prompt Learning\n",
    "\n",
    "Paper: https://arxiv.org/abs/2001.07676\n",
    "\n",
    "**Intuition**: Prompt learning is like giving the model a hint or a specific way to answer a question or complete a task, which helps it perform better without needing to be retrained from scratch.\n",
    "\n",
    "**Math**: Involves designing effective prompts that guide the model to produce the desired output. This can be seen as conditioning the model's responses on specific input patterns.\n",
    "\n",
    "**Simple Implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830886f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate English to French: Hello, how are you?\n",
      "\n",
      "Hello, how are you? Translate English to Spanish: Hello, how are you?\n",
      "\n",
      "Hello, how are you? Translate English to Portuguese: Hello, how are\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "prompt = \"Translate English to French: Hello, how are you?\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f428ba6",
   "metadata": {},
   "source": [
    "These methods represent the forefront of research and development in transformer models, focusing on improving efficiency, scalability, and performance in various tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ccb5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
